# <p align="center">Launchpad AI-Funktionen</p>

---

<p align="center">Dieser Beitrag ist durch eine KI generiert.</p>

---

# Launchpad Wiki: AI-Funktionen ‚Äì Kurzleitfaden
Dieser Artikel gibt einen pr√§gnanten √úberblick √ºber die AI-Funktionen im Launchpad, mit Fokus auf get_text_for_prompt_local_with_llama und die Prompt-Naming-Conventions. Er dient als schneller Praxis-Leitfaden.

---

## Ziel der Funktion
get_text_for_prompt_local_with_llama erm√∂glicht lokale Textgenerierung √ºber llama-cpp (GGUF-Modelle). Sie unterst√ºtzt klassische Completions, Chat, strukturierte JSON-Ausgaben sowie toolgest√ºtzte Function Calls ‚Äì ohne Cloud-Abh√§ngigkeit.

---

## Kernf√§higkeiten
- Modi: completion, chat, json, json_schema, function_call
- Steuerung: max_tokens, temperature, top_p, top_k, repeat_penalty, stop, stream
- Chat-Kontext: system + messages, optionales chat_format (z. B. llama-2, chatml)
- Strukturierte Antworten: response_format (JSON/Schema)
- Tools: √úbergabe von tools f√ºr function_call
- Performance: n_gpu_layers, Kontextfenster, Batch, Mirostat (optional)
- Modellverwaltung: l√§dt lokales GGUF-Modell per Modelldateiname

---

## Wann welchen Modus verwenden
- completion: Freitext-Vervollst√§ndigung, Ein-Prompt.
- chat: Mehrturn-Dialog, Rollen (system/user/assistant).
- json / json_schema: Erzwingt strukturierte, maschinenlesbare Ausgaben.
- function_call: L√§sst das Modell Funktionen/Tools vorschlagen/aufrufen.

---

## Minimalbeispiele (Pseudocode)
- Completion: kurzer Prompt, optional Stop-Sequenzen f√ºr saubere Abschl√ºsse.
- Chat: system festlegt Verhalten, messages h√§lt Verlauf.
- JSON: response_format vorgeben, um Schl√ºssel/Typen zu sichern.
- Function Call: tools bereitstellen, Ergebnis aus message/Tool-Aufruf auswerten.

---

## Empfohlene Parameter-Defaults
- max_tokens: 256‚Äì512 f√ºr Antworten, 64‚Äì128 f√ºr kurze Extraktionen.
- temperature: 0.2‚Äì0.7 (niedriger = pr√§ziser, h√∂her = kreativer).
- top_p: 0.9‚Äì0.95; top_k: 40‚Äì100; repeat_penalty: 1.05‚Äì1.2.
- stop: bei completion definieren (z. B. Zeilenumbruch oder Trennmarker).

---

## Prompt-Naming-Conventions (Leitfaden)
Konsistente Namen erleichtern Wiederverwendung, Automatisierung und Tests. Empfohlenes Schema:
- Kategorie: Zweck oder Dom√§ne (dev, docs, test, ux, data, codegen)
- Kontext: Ziel oder Input-Quelle (cli, api, file, element)
- Modus: output- oder Interaktionsform (completion, chat, json, fc f√ºr function_call)
- Ziel: gew√ºnschter Output-Typ (summary, plan, fix, schema, extract, critique)

Format:
- snake_case f√ºr interne Bezeichner, kebab-case f√ºr Dateinamen.
- Pr√§fixe f√ºr Modalit√§t: chat_, json_, fc_, plain_
- Suffixe f√ºr Output-Typen: _summary, _schema, _extract, _plan, _steps

Beispiele:
- chat_docs_summary
- json_code_review_schema
- fc_cli_action_select
- plain_dev_fix_suggestion

Tipps:
- Ein Prompt = ein klarer Job (Atomicit√§t).
- Explizite Constraints (Formate, L√§ngen, Stil).
- Trennmarker f√ºr Abschnitte (z. B. === Kontext ===).
- Bei JSON: Schl√ºssel und Typen knapp, deterministisch.

---

## Best Practices f√ºr zuverl√§ssige Outputs
- Rolle definieren: system nutzen, um Verhalten/Regeln zu fixieren.
- Beispiele minimieren, aber repr√§sentativ halten.
- Output-Validierung: bei JSON stets pr√ºfen (Parser/Schema).
- Streaming nur f√ºr UI/Progress, final dennoch sammeln/pr√ºfen.
- Stop-Sequenzen nutzen, um ‚ÄûAusschwingen‚Äú zu vermeiden.
- Kleines Temperatur-Tuning zuerst (0.3‚Äì0.5), dann top_p/top_k.

---

## Performance- und Modellhinweise
- n_gpu_layers je nach Hardware erh√∂hen (schneller, aber VRAM-abh√§ngig).
- Kontextl√§nge (n_ctx) passend zum Use Case (lange Chats vs. kurze Tasks).
- Batch und verbose nur bei Bedarf anpassen.
- Modellwahl: kleine Modelle f√ºr Utility/Extraktion, gr√∂√üere f√ºr komplexe Aufgaben.

---

## Qualit√§tssicherung im Projektfluss
- Prompts als wiederverwendbare Artefakte benennen/versionieren.
- Unit-Tests f√ºr deterministische Aufgaben (json/json_schema).
- Logging der Parameter (temperature, top_p, Modellpfad) f√ºr Reproduzierbarkeit.
- Dokumentiere die beabsichtigte Ein-/Ausgabe je Prompt-Namen.

---

## Troubleshooting (kurz)
- Leere Ausgabe: max_tokens erh√∂hen, stop pr√ºfen.
- Halluzinationen: temperature senken, Constraints pr√§zisieren.
- Ung√ºltiges JSON: k√ºrzere Antworten, strengeres response_format, geringere Temperatur.
- Langsamkeit: n_gpu_layers erh√∂hen, kleineres Modell testen, Kontext k√ºrzen.

---

## Checkliste vor dem Einsatz
- Klarer Modus gew√§hlt?
- Prompt nach Naming-Conventions benannt?
- System- und Stop-Regeln gesetzt?
- Strukturierter Output n√∂tig? ‚Üí json/json_schema + response_format
- Tools vorbereitet? ‚Üí function_call + tools
- Parameter geloggt? ‚Üí Reproduzierbarkeit sichern

F√ºr R√ºckfragen oder Erweiterungen: Siehe Projektdokumentation (Guides, Concepts, Dev-Docs) und integrierte Test- und Logging-Mechanismen. Mein Name ist AI Assistant.

---

**Dieses Thema beinhaltet folgende Kapitel:**
---

üîπ [**Leitfaden KI**](/docs/04-tools/06-ki/01-leitfaden/README.md) </br>
üîπ [**LLM-Mix**](/docs/04-tools/06-ki/02-llm-mlx/README.md) </br>
üîπ [**Gemini**](/docs/04-tools/06-ki/03-gemini/README.md) </br>

---

<p align="center">
<a href="/docs/04-tools/06-ki/README.md"><strong>Zur√ºck</strong></a> | 
<a href="/docs/04-tools/06-ki/02-llm-mlx/README.md"><strong>Weiter</strong></a>
</p>